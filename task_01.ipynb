{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "#### \n",
    "import nltk\n",
    "# to trzeba ściągnąć, w cmd:\n",
    "# >>> import nltk\n",
    "# >>> nltk.download()\n",
    "# For central installation, set this to C:\\nltk_data\n",
    "\n",
    "import itertools\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# transformatory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# estymator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# splitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych tekstowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuwanie znaków interpunkcyjnych, takich jak:\n",
    "'!\"#$%&\\'()*+,-./:;&lt;=>?@[\\\\]^_`{|}~'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zbiór znaków interpunkcyjnych\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncation(text):\n",
    "    cleaned = ''.join([word for word in text if word not in string.punctuation])\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizacja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    # Usunięcie wielkich liter\n",
    "    clean_text = text.lower()\n",
    "\n",
    "    # Tokenizacja\n",
    "    tokenized_text = nltk.word_tokenize(clean_text)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuwanie stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    without_stopwords = [word for word in text if word not in stopwords]\n",
    "    return without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "inaczej: proces tworzenia morfologicznych słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatter = nltk.WordNetLemmatizer()\n",
    "def lemmatizing(text):\n",
    "    lemmatized_words = [lemmatter.lemmatize(word) for word in text]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surowe dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Spam    5572 non-null   object\n",
      " 1   Text    5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "spam_dataset = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\", usecols=[0, 1], names=['Spam', 'Text'],\n",
    "                           skiprows=1)\n",
    "spam_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spam                                               Text\n",
       "0     0  Go until jurong point, crazy.. Available only ...\n",
       "1     0                      Ok lar... Joking wif u oni...\n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset['Spam'] = spam_dataset['Spam'].replace(['ham','spam'],[0,1])\n",
    "spam_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam\n",
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset['Spam'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwanie znaków interpunkcyjnych, takich jak: '!\"#$%&\\'()*+,-./:;&lt;=>?@[\\\\]^_`{|}~'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset['Cleaned_Text'] = spam_dataset['Text'].apply(lambda x: remove_puncation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset['Tokenized_Text'] = spam_dataset['Cleaned_Text'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwanie stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset['WithoutStop_Text'] = spam_dataset['Tokenized_Text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming (nie zastosowano, operacja ta \"psuje\" dane). Nie działa dokładnie tak, jak na stronie kursu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam_dataset['Stemmed_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: stemming(x))\n",
    "spam_dataset['Stemmed_Text'] = spam_dataset['WithoutStop_Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>WithoutStop_Text</th>\n",
       "      <th>Stemmed_Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spam                                               Text  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "\n",
       "                                        Cleaned_Text  \\\n",
       "0  Go until jurong point crazy Available only in ...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "\n",
       "                                      Tokenized_Text  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "\n",
       "                                    WithoutStop_Text  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "\n",
       "                                        Stemmed_Text  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "\n",
       "                                     Lemmatized_Text  \n",
       "0  [go, jurong, point, crazy, available, bugis, n...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset['Lemmatized_Text'] = spam_dataset['Stemmed_Text'].apply(lambda x: lemmatizing(x))\n",
    "spam_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usunięcie etapów pośrednich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spam_dataset.drop(['Cleaned_Text', 'Tokenized_Text', 'WithoutStop_Text', 'Stemmed_Text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spam                                               Text  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     0  U dun say so early hor... U c already then say...   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     Lemmatized_Text  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, go, usf, life, around, though]   \n",
       "\n",
       "                                            features  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf life around though  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['features'] = data.Lemmatized_Text.apply(lambda x: \" \".join(x))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane wejściowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.features, data.Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model wstepny - obliczenie feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline - jakie n-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestestimator(transformer):\n",
    "    cv = KFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    classifier = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "    pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                                        ('classifier', classifier)])\n",
    "\n",
    "    params = {'transformer__min_df': [0.001, 0.01],\n",
    "            'transformer__max_df': [0.25, 0.5, 0.75],\n",
    "            'transformer__ngram_range': [(1, 1),(1,2),(2,2)]}\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipeline,\n",
    "                                params,\n",
    "                                scoring='f1',\n",
    "                                cv=cv,\n",
    "                                n_jobs=-1)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    print(\"\\nNajlepsze hiperparametry:\", gridsearch.best_params_, \"\\n\")\n",
    "\n",
    "    model_best = gridsearch.best_estimator_\n",
    "    \n",
    "    return model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Najlepsze hiperparametry: {'transformer__max_df': 0.25, 'transformer__min_df': 0.01, 'transformer__ngram_range': (1, 1)} \n",
      "\n",
      "Score on train data: 0.8721055465805062,\n",
      "score on test data: 0.8735199138858988\n"
     ]
    }
   ],
   "source": [
    "model_count_best = bestestimator(transformer = CountVectorizer())\n",
    "print(f'Score on train data: {model_count_best.score(X_train, y_train)},\\nscore on test data: {model_count_best.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Najlepsze hiperparametry: {'transformer__max_df': 0.25, 'transformer__min_df': 0.01, 'transformer__ngram_range': (1, 1)} \n",
      "\n",
      "Score on train data: 0.8729133010231557,\n",
      "score on test data: 0.8740581270182992\n"
     ]
    }
   ],
   "source": [
    "model_tfid_best = bestestimator(transformer=TfidfVectorizer())\n",
    "print(f'Score on train data: {model_tfid_best.score(X_train, y_train)},\\nscore on test data: {model_tfid_best.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybieram transformer TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8729133010231557, 0.8740581270182992)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.25, ngram_range=(1,1))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "classifier.score(X_train_vectorized, y_train), classifier.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([[classifier.score(X_train_vectorized, y_train), classifier.score(X_test_vectorized, y_test)]], columns=['train_score', 'test_score'], index=['tfidf_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf_raw</th>\n",
       "      <td>0.872913</td>\n",
       "      <td>0.874058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           train_score  test_score\n",
       "tfidf_raw     0.872913    0.874058"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczam feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 2.09358737e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.00766848e-04, 0.00000000e+00,\n",
       "       1.51801572e-01, 1.66151412e-04, 0.00000000e+00, 2.75148970e-02,\n",
       "       8.24331376e-02, 0.00000000e+00, 1.98292835e-03, 3.86128000e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.feature_importances_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find': 25,\n",
       " 'tell': 100,\n",
       " 'buy': 7,\n",
       " 'oh': 73,\n",
       " 'cant': 9,\n",
       " 'every': 23,\n",
       " 'day': 18,\n",
       " 'know': 46,\n",
       " 'get': 28,\n",
       " 'babe': 5,\n",
       " 'go': 30,\n",
       " 'ive': 44,\n",
       " 'like': 52,\n",
       " 'im': 43,\n",
       " 'phone': 77,\n",
       " 'free': 26,\n",
       " 'call': 8,\n",
       " 'tc': 99,\n",
       " 'dear': 19,\n",
       " 'good': 33,\n",
       " 'morning': 64,\n",
       " 'said': 86,\n",
       " 'amp': 2,\n",
       " 'msg': 65,\n",
       " 'even': 22,\n",
       " 'got': 34,\n",
       " 'back': 6,\n",
       " 'still': 95,\n",
       " 'today': 108,\n",
       " 'send': 89,\n",
       " 'min': 61,\n",
       " 'dont': 21,\n",
       " 'pls': 81,\n",
       " 'need': 68,\n",
       " 'come': 14,\n",
       " 'hi': 39,\n",
       " 'want': 115,\n",
       " 'work': 123,\n",
       " 'reply': 84,\n",
       " 'co': 13,\n",
       " 'life': 51,\n",
       " 'lot': 55,\n",
       " 'love': 56,\n",
       " 'friend': 27,\n",
       " 'tomorrow': 109,\n",
       " 'lor': 54,\n",
       " 'sorry': 94,\n",
       " 'ill': 42,\n",
       " 'later': 49,\n",
       " 'ask': 4,\n",
       " 'message': 60,\n",
       " 'please': 80,\n",
       " 'stop': 96,\n",
       " 'one': 75,\n",
       " 'mobile': 63,\n",
       " 'service': 91,\n",
       " 'per': 76,\n",
       " 'wish': 121,\n",
       " 'yes': 127,\n",
       " 'well': 119,\n",
       " 'way': 117,\n",
       " 'anything': 3,\n",
       " 'ltgt': 57,\n",
       " 'take': 98,\n",
       " 'contact': 15,\n",
       " 'da': 17,\n",
       " 'sure': 97,\n",
       " 'ur': 112,\n",
       " 'hope': 41,\n",
       " 'care': 10,\n",
       " 'text': 101,\n",
       " 'make': 58,\n",
       " 'home': 40,\n",
       " 'night': 71,\n",
       " 'going': 31,\n",
       " 'time': 107,\n",
       " 'hey': 38,\n",
       " 'guy': 36,\n",
       " 'number': 72,\n",
       " 'see': 88,\n",
       " 'txt': 111,\n",
       " 'think': 105,\n",
       " 'show': 92,\n",
       " 'late': 48,\n",
       " 'ok': 74,\n",
       " 'also': 1,\n",
       " 'new': 69,\n",
       " 'last': 47,\n",
       " 'cash': 11,\n",
       " 'claim': 12,\n",
       " 'thats': 103,\n",
       " 'give': 29,\n",
       " 'gon': 32,\n",
       " 'na': 67,\n",
       " 'thanks': 102,\n",
       " 'great': 35,\n",
       " 'feel': 24,\n",
       " 'thing': 104,\n",
       " 'year': 126,\n",
       " 'place': 79,\n",
       " 'already': 0,\n",
       " 'happy': 37,\n",
       " 'wan': 114,\n",
       " 'would': 124,\n",
       " 'say': 87,\n",
       " 'next': 70,\n",
       " 'week': 118,\n",
       " 'prize': 82,\n",
       " 'urgent': 113,\n",
       " 'really': 83,\n",
       " 'miss': 62,\n",
       " 'tonight': 110,\n",
       " 'wat': 116,\n",
       " 'yeah': 125,\n",
       " 'wont': 122,\n",
       " 'win': 120,\n",
       " 'something': 93,\n",
       " 'youre': 128,\n",
       " 'keep': 45,\n",
       " 'much': 66,\n",
       " 'didnt': 20,\n",
       " 'lol': 53,\n",
       " 'let': 50,\n",
       " 'thought': 106,\n",
       " 'pick': 78,\n",
       " 'meet': 59,\n",
       " 'could': 16,\n",
       " 'right': 85,\n",
       " 'sent': 90}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model docelowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozostawiam cechy o ważności $10^{-3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  8,  11,  12,  14,  15,  23,  26,  42,  43,  60,  61,  63,  65,\n",
       "         69,  76,  80,  82,  84,  89,  91,  92,  94,  96,  99, 101, 111,\n",
       "        112, 113, 118, 120], dtype=int64),)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_leave = np.where(classifier.feature_importances_>1e-3)\n",
    "features_to_leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get': 28,\n",
       " 'ive': 44,\n",
       " 'like': 52,\n",
       " 'phone': 77,\n",
       " 'free': 26,\n",
       " 'msg': 65,\n",
       " 'back': 6,\n",
       " 'love': 56,\n",
       " 'friend': 27,\n",
       " 'way': 117,\n",
       " 'anything': 3,\n",
       " 'take': 98,\n",
       " 'da': 17,\n",
       " 'care': 10,\n",
       " 'hey': 38,\n",
       " 'txt': 111,\n",
       " 'show': 92,\n",
       " 'ok': 74,\n",
       " 'claim': 12,\n",
       " 'give': 29,\n",
       " 'gon': 32,\n",
       " 'thanks': 102,\n",
       " 'feel': 24,\n",
       " 'place': 79,\n",
       " 'happy': 37,\n",
       " 'tonight': 110,\n",
       " 'wat': 116,\n",
       " 'yeah': 125,\n",
       " 'keep': 45,\n",
       " 'didnt': 20}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_trimmed = dict(zip(list(np.array(list(vectorizer.vocabulary_.keys()))[features_to_leave]),\n",
    "    list(np.array(list(vectorizer.vocabulary_.values()))[features_to_leave])))\n",
    "vocabulary_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'ive', 'like', 'phone', 'free', 'msg', 'back', 'love', 'friend', 'way', 'anything', 'take', 'da', 'care', 'hey', 'txt', 'show', 'ok', 'claim', 'give', 'gon', 'thanks', 'feel', 'place', 'happy', 'tonight', 'wat', 'yeah', 'keep', 'didnt']\n"
     ]
    }
   ],
   "source": [
    "words_left = list(np.array(list(vectorizer.vocabulary_.keys()))[features_to_leave])\n",
    "print(words_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"wyczyszczone\" listy słów z poszczególnych dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "      <th>features</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>[wat]</td>\n",
       "      <td>wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok]</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, txt]</td>\n",
       "      <td>free txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å£750, po...</td>\n",
       "      <td>2nd time tried 2 contact u u å£750 pound prize...</td>\n",
       "      <td>[claim]</td>\n",
       "      <td>claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>[ì, b, going, esplanade, fr, home]</td>\n",
       "      <td>ì b going esplanade fr home</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[pity, mood, soany, suggestion]</td>\n",
       "      <td>pity mood soany suggestion</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "      <td>guy bitching acted like id interested buying s...</td>\n",
       "      <td>[like, free]</td>\n",
       "      <td>like free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>rofl true name</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                               Text  \\\n",
       "0        0  Go until jurong point, crazy.. Available only ...   \n",
       "1        0                      Ok lar... Joking wif u oni...   \n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        0  U dun say so early hor... U c already then say...   \n",
       "4        0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567     1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568     0              Will Ì_ b going to esplanade fr home?   \n",
       "5569     0  Pity, * was in mood for that. So...any other s...   \n",
       "5570     0  The guy did some bitching but I acted like i'd...   \n",
       "5571     0                         Rofl. Its true to its name   \n",
       "\n",
       "                                        Lemmatized_Text  \\\n",
       "0     [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, early, hor, u, c, already, say]   \n",
       "4     [nah, dont, think, go, usf, life, around, though]   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å£750, po...   \n",
       "5568                 [ì, b, going, esplanade, fr, home]   \n",
       "5569                    [pity, mood, soany, suggestion]   \n",
       "5570  [guy, bitching, acted, like, id, interested, b...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                               features         clean  \\\n",
       "0     go jurong point crazy available bugis n great ...         [wat]   \n",
       "1                               ok lar joking wif u oni          [ok]   \n",
       "2     free entry 2 wkly comp win fa cup final tkts 2...   [free, txt]   \n",
       "3                   u dun say early hor u c already say            []   \n",
       "4              nah dont think go usf life around though            []   \n",
       "...                                                 ...           ...   \n",
       "5567  2nd time tried 2 contact u u å£750 pound prize...       [claim]   \n",
       "5568                        ì b going esplanade fr home            []   \n",
       "5569                         pity mood soany suggestion            []   \n",
       "5570  guy bitching acted like id interested buying s...  [like, free]   \n",
       "5571                                     rofl true name            []   \n",
       "\n",
       "     clean_features  \n",
       "0               wat  \n",
       "1                ok  \n",
       "2          free txt  \n",
       "3                    \n",
       "4                    \n",
       "...             ...  \n",
       "5567          claim  \n",
       "5568                 \n",
       "5569                 \n",
       "5570      like free  \n",
       "5571                 \n",
       "\n",
       "[5572 rows x 6 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean'] = data['Lemmatized_Text'].apply(lambda x: [j for j in x if j in words_left])\n",
    "data['clean_features'] = data['clean'].apply(lambda x: \" \".join(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czyszczę DataFrame z pustych cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['clean_features'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "      <th>features</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>[wat]</td>\n",
       "      <td>wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok]</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, txt]</td>\n",
       "      <td>free txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spam                                               Text  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "\n",
       "                                     Lemmatized_Text  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "\n",
       "                                            features        clean  \\\n",
       "0  go jurong point crazy available bugis n great ...        [wat]   \n",
       "1                            ok lar joking wif u oni         [ok]   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  [free, txt]   \n",
       "\n",
       "  clean_features  \n",
       "0            wat  \n",
       "1             ok  \n",
       "2       free txt  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba danych spada o ponad połowę"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dane treningowe i testowe z nowego zbioru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.clean_features, data.Spam\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obliczenia dla wybranych parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8460166468489893, 0.8406658739595719)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.25, ngram_range=(1,1))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "classifier.score(X_train_vectorized, y_train), classifier.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docelowy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "transformer = CountVectorizer(min_df=0.01, max_df=0.25, ngram_range=(1,1))\n",
    "# transformer = TfidfVectorizer(min_df=0.01, max_df=0.25, ngram_range=(1,1))\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                            ('classifier', classifier)])\n",
    "\n",
    "# pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Najlepsze hiperparametry: {'transformer__max_df': 0.4, 'transformer__min_df': 0.0001, 'transformer__ngram_range': (1, 1)} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'max_df corresponds to < documents than min_df'\n",
    "\n",
    "params = {'transformer__min_df': [0.0001, 0.001, 0.005],\n",
    "        # 'transformer__max_df': [0.4, 0.5, 0.6, 0.7, 0.8, 0.99, 1.0],\n",
    "        'transformer__max_df': [0.4, 0.6, 0.8, 0.99, 1.0],\n",
    "        'transformer__ngram_range': [(1,1),(1,2),(2,2)]}\n",
    "\n",
    "gridsearch = GridSearchCV(pipeline,\n",
    "                            params,\n",
    "                            scoring='f1',\n",
    "                            cv=cv,\n",
    "                            n_jobs=-1)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"\\nNajlepsze hiperparametry:\", gridsearch.best_params_, \"\\n\")\n",
    "\n",
    "model_best = gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocena docelowego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8460166468489893, 0.8406658739595719)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best.score(X_train, y_train), model_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.DataFrame([[model_best.score(X_train, y_train), model_best.score(X_test, y_test)]], columns=['train_score', 'test_score'], index=['optimized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, new_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf_raw</th>\n",
       "      <td>0.872913</td>\n",
       "      <td>0.874058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimized</th>\n",
       "      <td>0.846017</td>\n",
       "      <td>0.840666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           train_score  test_score\n",
       "tfidf_raw     0.872913    0.874058\n",
       "optimized     0.846017    0.840666"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik jest porównywalny, biorąc pod uwagę, że trochę danych wypadło."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
